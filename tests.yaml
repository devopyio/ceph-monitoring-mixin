rule_files:
  - prometheus_alerts.yaml

evaluation_interval: 1m

tests:
- interval: 1m
  input_series:
  - series: 'ceph_health_status{job="ceph",instance="instance:9283"}'
    values: '0 1'
  alert_rule_test:
  - eval_time: 1m
    alertname: CephHealthStatusWarn
    exp_alerts:
    - exp_labels:
        job: ceph
        instance: 'instance:9283'
        severity: warning
      exp_annotations:
        message: 'Ceph Healh status is WARN.'
        component: general
        runbook_url: https://github.com/devopyio/ceph-monitoring-mixin/tree/master/runbook.md#alert-name-cephhealthstatuswarn
        grafana_url: ''

- interval: 1m
  input_series:
  - series: 'ceph_health_status{job="ceph",instance="instance:9283"}'
    values: '0 2'
  alert_rule_test:
  - eval_time: 1m
    alertname: CephHealthStatusErr
    exp_alerts:
    - exp_labels:
        job: ceph
        instance: 'instance:9283'
        severity: critical
      exp_annotations:
        message: 'Ceph Healh status is ERR.'
        component: general
        runbook_url: https://github.com/devopyio/ceph-monitoring-mixin/tree/master/runbook.md#alert-name-cephhealthstatuserr
        grafana_url: ''

- interval: 1m
  input_series:
  - series: 'ceph_pg_clean{job="ceph",instance="instance:9283"}'
    values: '128 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100 100'
  - series: 'ceph_pg_total{job="ceph",instance="instance:9283"}'
    values: '128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128 128'
  alert_rule_test:
  - eval_time: 31m
    alertname: CephPGAreUnclean
    exp_alerts:
    - exp_labels:
        job: ceph
        instance: 'instance:9283'
        severity: warning
      exp_annotations:
        message: 'Placement groups contain objects that are not replicated the desired number of times. They should be recovering.'
        component: 'ceph-osd'
        runbook_url: https://github.com/devopyio/ceph-monitoring-mixin/tree/master/runbook.md#alert-name-cephpgareunclean
        grafana_url: ''

- interval: 1m
  input_series:
  - series: 'ceph_osd_stat_bytes_used{job="ceph",instance="instance:9283",ceph_daemon="osd.0"}'
    values: '800 950'
  - series: 'ceph_osd_stat_bytes{job="ceph",instance="instance:9283",ceph_daemon="osd.0"}'
    values: '1024 1024'
  alert_rule_test:
  - eval_time: 2m
    alertname: CephOSDLowSpace
    exp_alerts:
    - exp_labels:
        job: ceph
        instance: 'instance:9283'
        ceph_daemon: 'osd.0'
        severity: warning
      exp_annotations:
        message: 'osd.0 Ceph OSD used more than 85 % of disk space.'
        component: 'ceph-osd'
        runbook_url: https://github.com/devopyio/ceph-monitoring-mixin/tree/master/runbook.md#alert-name-cephosdlowspace
        grafana_url: ''
